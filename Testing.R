## ----setup, include=FALSE, message=FALSE---------------------------------------------
# library(tidyverse)
# library(nimble)
# library(coda)
# library(rstan)
# library(parallel)
# library(here)
# library(kableExtra)

devtools::load_all()

## ------------------------------------------------------------------------------------
psi <- c(0.3, 0.6)
lambda <- c(11, 2)




# Define sites and visits
nspecies <- length(psi)
nsites <- 30
nvisits <- 5


## ------------------------------------------------------------------------------------
test_theta1 <- matrix(c(0.9, 0.1, 0.15, 0.85),
                      byrow = TRUE, nrow = 2)
test_theta1


## ------------------------------------------------------------------------------------
# Generating a confusion matrix in this way appears to be the culprit of the testing
# problems; this doesn't necessarily give exactly 1, but values that are extremely close.

test_theta2 <- t(apply(18*diag(nspecies) + 2, 1,
                       function(x) nimble::rdirch(alpha = x)))
test_theta2


## ------------------------------------------------------------------------------------
## Now a built in test within the simulate_ValidatedData function, per Katie's comment
rowSums(test_theta1)
rowSums(test_theta2)


## ------------------------------------------------------------------------------------

val_scenarios <- list(spp1 = c(.75, .5), spp2 = .5)

## ----message=FALSE-------------------------------------------------------------------
fake_data <- simulate_validatedData(
  n_datasets = 5,
  design_type = "BySpecies",
  scenarios = val_scenarios,
  nsites = nsites,
  nvisits = nvisits,
  nspecies = nspecies,
  psi = psi,
  lambda = lambda,
  theta = test_theta1,
  save_datasets = FALSE,
  save_masked_datasets = FALSE,
  directory = paste0(here::here("Testing"))
)

## ------------------------------------------------------------------------------------
# investigate the validation scenarios created by enumerating the species validation levels
fake_data$scenarios_df

# investigate the number of recordings validated under each scenario
validation_summary <- summarize_n_validated(
  data_list = fake_data$masked_dfs,
  scenario_names = as.character(1:nrow(fake_data$scenarios_df)),
  theta_scenario = "1"
)

validation_summary

#example_val_sum <- validation_summary
#usethis::use_data(example_val_sum)

## ------------------------------------------------------------------------------------
full_dfs <- fake_data$full_datasets
head(full_dfs[[1]])
# WHY ARE THE ROWS REPEATED?  E.G., ROW 1 AND ROW 2 ARE THE EXACT SAME... SO ARE
# ROWS 3-6...

# Jacob's answer: the rows are repeated because they were generated by the same species
# and were assigned the same autoID. You'll notice the count column will say (for example,
# in the situation you noted) that count = 2 for rows 1-2, but then count = 4 for rows 3-6.
# That helps with bookkeeping, because it tells the user that there were (for example) two
# calls from  species 1 that were autoID'd to 1 (rows 1 and 2), and there were four calls
# from species  1 that were autoID'd to 2 (rows 3-6). This is the confusing thing about
# having the data at the individual call level -- there will always be repeated rows unless
# each autoID/true species combination is observed exactly once on a visit to a site.


## ------------------------------------------------------------------------------------
site_visits_without_calls <- fake_data$zeros
head(site_visits_without_calls[[1]]) # notice that counts = 0 for all rows


## ------------------------------------------------------------------------------------
masked_dfs <- fake_data$masked_dfs

# View dataset 1 with scenario 1 validation effort.
# HOW DOES THIS CONNECT TO FULL_DFS? LOOKS LIKE IT'S THE DISAGGREGATED
# VERSION, AND MAYBE THE FIRST 2 ROWS OF MASKED DFS REPRESENT 2 OF THE 5 CALLS THAT
# WERE CLASSIFIED FROM SPP 3 TO SPP 3 AT SITE 1 VISIT 1, BUT THEN WHY ARE THERE ONLY 3 ROWS
# AND NOT 5 IN TOTAL FROM THIS SITE-VISIT?
# HARD TO REALITY CHECK THIS CODE WITHOUT KNOWING HOW IT CONNECTS TO FULL_DFS

head(masked_dfs[[1]][[1]])

# Jacob's answer: This is a copy of full_dfs[[1]] subject to the masking of true species labels
# according to scenario 2. I realized that there was a mismatch between the full (unmasked) df
# above and the masked dataset shown here.

## -------------------------------------------------------------------------------------
## Testing for the MCMC_tuning function: use the "worst case" scenario, which is scenario 1
## for these scenarios because it has the lowest average number of calls validated per
## dataset

tuning_list <- tune_mcmc(dataset = masked_dfs[[1]][[5]], zeros = fake_data$zeros[[5]])

min_iters <- tuning_list$min_iter
warmup <- tuning_list$min_warmup
expected_time <- tuning_list$max_iter_time

# This output shows a matrix. A value of 1 in a cell indicates that the combination
# of warmup + iters yielded Rhat < 1.1 for all model parameters.
tuning_list$convergence_matrix

## -------------------------------------------------------------------------------------
# Run time will vary: with 5 datasets, 30 sites, 5 visits, 2 species and the assigned
# psi and lambda values, this takes ~ 1:30 per scenario. With the 2 scenarios above,
# this amounts to ~ 3 minutes when fitting in parallel.

sims_output <- run_sims(
         data_list = fake_data$masked_dfs,
         zeros_list = fake_data$zeros,
         DGVs = list(lambda = lambda, psi = psi, theta = test_theta1),
         theta_scenario_id = 1,
         parallel = TRUE,
         niter = min_iters,
         nburn = warmup,
         thin = 1,
         save_fits = FALSE,
         save_individual_summaries_list = FALSE,
         directory = here::here("Testing")
)

# example_output <- sims_output
# usethis::use_data(example_output)

## ------------------------------------------------------------------------------------
visualize_parameter_group(sim_summary = sims_output,
                          pars = "lambda",
                          theta_scenario = 1,
                          scenarios = 1:2,
                          convergence_threshold = 1.1)


## ------------------------------------------------------------------------------------
# note the space between the indices for theta[2, 1]
visualize_single_parameter(sims_output, par = "lambda[2]",
                           theta_scenario = 1,
                           scenarios = 1:2,
                           convergence_threshold = 1.03)

plot_coverage_vs_calls(
  sims_output,
  validation_summary,
  regex_pars = "lambda",
  scenarios = 1:2,
  theta_scenario = 1,
  convergence_threshold = 1.1
)

plot_bias_vs_calls(
  sims_output,
  validation_summary,
  pars = c("psi[1]", "psi[2]", "psi[3]"),
  scenarios = 1:8,
  theta_scenario = 1,
  convergence_threshold = 1.1
)

plot_width_vs_calls(
  sims_output,
  validation_summary,
  pars = c("lambda[1]", "psi[1]"),
  scenarios = 1:2,
  theta_scenario = 1,
  convergence_threshold = 1.05
)

## ----message=FALSE-------------------------------------------------------------------
psi <- c(0.633, 0.612, 0.849)
lambda <- c(5.934, 4.160, 14.25)

nspecies <- length(psi)
nsites <- 30
nvisits <- 4

Theta_FE <- t(apply(12*diag(nspecies) + 2, 1, function(x) nimble::rdirch(alpha = x)))

FE_data <- simulate_validatedData(
  n_datasets = 5,
  validation_design = "FixedPercent",
  scenarios = c(0.05, .35, 0.65), # Note this is now a **vector** of possible scenarios. Also note the extremely small level of validation effort in the first entry!
  nsites = nsites,
  nvisits = nvisits,
  nspecies = nspecies,
  psi = psi,
  lambda = lambda,
  theta = Theta_FE,
  save_datasets = FALSE,
  save_masked_datasets = FALSE,
  directory = here("Testing", "FixedEffortExample")
)


## ------------------------------------------------------------------------------------
# Runtime: with the specified number of sites, visits, species, validation scenarios
# and parameter settings, this takes ~ 2:20 minutes per scenario, so around 6-7 minutes for
# this small test case. Plan accordingly!

start <- Sys.time()
FE_model_fits <- run_sims(
  data_list = FE_data$masked_dfs,
  zeros_list = FE_data$zeros,
  theta_scenario_id = "FE",
  save_fits = FALSE,
  DGVs = list(lambda = lambda, psi = psi, theta = Theta_FE),
  save_individual_summaries_list = FALSE,
  directory = here("Testing", "FixedEffortExample")
)

Sys.time() - start


## ------------------------------------------------------------------------------------
visualize_parameter_group(FE_model_fits, pars = "lambda", theta_scenario = 1, scenarios = 1:3)


## ------------------------------------------------------------------------------------
visualize_parameter_group(FE_model_fits, pars = "theta", theta_scenario = 1, scenarios = 1:3)


## ------------------------------------------------------------------------------------
summarize_n_validated(FE_data$masked_dfs, theta_scenario = "FE", scenario_names = as.character(1:3))

## ------------------------------------------------------------------------------------
plot_bias_vs_calls(
  FE_model_fits,
  summarize_n_validated(FE_data$masked_dfs, theta_scenario = "FE", scenario_names = as.character(1:3)),
  theta_scenario = 1,
  scenarios = 1:4, convergence_threshold = 1.1
)

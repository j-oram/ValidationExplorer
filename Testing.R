## ----setup, include=FALSE, message=FALSE---------------------------------------------
library(tidyverse)
library(nimble)
library(coda)
library(rstan)
library(parallel)
library(here)
library(kableExtra)
theme_set(theme_bw())

## ----eval=FALSE, echo=TRUE-----------------------------------------------------------
## install.packages("your_package_name_here")


## ------------------------------------------------------------------------------------
# For simulation 
source("Data Simulation/simulate_validatedData.R")
source("Data Simulation/count_detection_sim.R")
source("Data Simulation/mask_by_spp.R")
source("Data Simulation/mask_FE.R")
source("Data Simulation/summarize_n_validated.R")

# For model fitting/MCMC
source("Model Fitting & Simulation/run_sims.R")
source("Model Fitting & Simulation/MCMC_sum.R")
source("Model Fitting & Simulation/runMCMC_fit.R")

# Visualization
source("Summary Figures/visualize_sims.R")

## ------------------------------------------------------------------------------------
psi <- c(0.3, 0.6)
lambda <- c(11, 2)

# Define sites and visits 
nspecies <- length(psi)
nsites <- 30
nvisits <- 5


## ------------------------------------------------------------------------------------
test_theta1 <- matrix(c(0.9, 0.1, 0.15, 0.85), byrow = TRUE, nrow = 2)
test_theta1


## ------------------------------------------------------------------------------------
# Generating a confusion matrix in this way appears to be the culprit of the testing 
# problems; this doesn't necessarily give exactly 1, but values that are extremely close. 
test_theta2 <- t(apply(18*diag(nspecies) + 2, 1, function(x) nimble::rdirch(alpha = x)))
test_theta2


## ------------------------------------------------------------------------------------
## Now a built in test within the simulate_ValidatedData function, per Katie's comment
rowSums(test_theta1)
rowSums(test_theta2)


## ------------------------------------------------------------------------------------
# old way: do expand.grid yourself: 
#val_scenarios <- expand.grid(spp1 = c(.75, .5), spp2 = c(.25, .5), spp3 = c(.25, .75))

# simulate_ValidatedData now includes a call to expand.grid call internally, so all that 
# needs to be supplied is a list (when `validation_design = "BySpecies"`)

val_scenarios <- list(spp1 = c(.75, .5), spp2 = .5)

## ----message=FALSE-------------------------------------------------------------------
fake_data <- simulate_validatedData(
  n_datasets = 5, 
  validation_design = "BySpecies",
  scenarios = val_scenarios, 
  nsites = nsites, 
  nvisits = nvisits, 
  nspecies = nspecies,
  psi = psi, 
  lambda = lambda,
  theta = test_theta1, 
  save_datasets = FALSE,
  save_masked_datasets = FALSE,
  directory = paste0(here::here("Testing"))
)


## ------------------------------------------------------------------------------------
# investigate the validation scenarios created by enumerating the species validation levels
fake_data$scenarios_df

# investigate the number of recordings validated under each scenario
validation_summary <- summarize_n_validated(
  data_list = fake_data$masked_dfs, 
  scenario_names = as.character(1:nrow(fake_data$scenarios_df)), 
  theta_scenario = "1"
)

validation_summary

## ------------------------------------------------------------------------------------
full_dfs <- fake_data$full_datasets
head(full_dfs[[1]])
# WHY ARE THE ROWS REPEATED?  E.G., ROW 1 AND ROW 2 ARE THE EXACT SAME... SO ARE 
# ROWS 3-6...

# Jacob's answer: the rows are repeated because they were generated by the same species 
# and were assigned the same autoID. You'll notice the count column will say (for example,
# in the situation you noted) that count = 2 for rows 1-2, but then count = 4 for rows 3-6. 
# That helps with bookkeeping, because it tells the user that there were (for example) two 
# calls from  species 1 that were autoID'd to 1 (rows 1 and 2), and there were four calls 
# from species  1 that were autoID'd to 2 (rows 3-6). This is the confusing thing about 
# having the data at the individual call level -- there will always be repeated rows unless 
# each autoID/true species combination is observed exactly once on a visit to a site.  


## ------------------------------------------------------------------------------------
site_visits_without_calls <- fake_data$zeros
head(site_visits_without_calls[[1]])


## ------------------------------------------------------------------------------------
masked_dfs <- fake_data$masked_dfs

# View dataset 3 with scenario 7 validation effort.
# HOW DOES THIS CONNECT TO FULL_DFS? LOOKS LIKE IT'S THE DISAGGREGATED 
# VERSION, AND MAYBE THE FIRST 2 ROWS OF MASKED DFS REPRESENT 2 OF THE 5 CALLS THAT 
# WERE CLASSIFIED FROM SPP 3 TO SPP 3 AT SITE 1 VISIT 1, BUT THEN WHY ARE THERE ONLY 3 ROWS 
# AND NOT 5 IN TOTAL FROM THIS SITE-VISIT?
# HARD TO REALITY CHECK THIS CODE WITHOUT KNOWING HOW IT CONNECTS TO FULL_DFS

head(masked_dfs[[2]][[1]])

# Jacob's answer: This is a copy of full_dfs[[1]] subject to the masking of true species labels 
# according to scenario 2. I realized that there was a mismatch between the full (unmasked) df 
# above and the masked dataset shown here. 

# RECOMMEND A SMALLER TESTING EXAMPLE THAT TAKES <5 MINS, MAYBE JUST 2 OR 3 SCENARIOS?
# # started at 11:38:42.825776 finished at 2024-09-30 12:06:51 MDT
# Noted -- I changed it to two species, two scenarios, 5 datasets under each
## -------------------------------------------------------------------------------------
# Run time will vary: with 5 datasets, 30 sites, 5 visits, 2 species and the assigned 
# psi and lambda values, this takes ~ 1:30 per scenario. With the 2 scenarios above,
# this amounts to ~ 3 minutes when fitting in parallel. 

sims_output <- run_sims(
         data_list = fake_data$masked_dfs,
         zeros_list = fake_data$zeros,
         DGVs = list(lambda = lambda, psi = psi, theta = test_theta2),
         theta_scenario_id = 1, 
         parallel = TRUE,
         niter = 2000, thin = 1,
         save_fits = FALSE,
         save_individual_summaries_list = FALSE,
         directory = here("Testing")
)


## ------------------------------------------------------------------------------------
# read in fit object -- my sneaky way of making the vignette knit faster was to save the
# output from the previous chunk and then read in the results, hiding this code block
# fit_1_1 <- readRDS("../Testing/Theta1/fits/fit_1_1.rds")

# visualize using bayesplot (if `save_fits = TRUE`)
# bayesplot::mcmc_dens_overlay(fit_1_1, pars = "lambda[1]")


## ------------------------------------------------------------------------------------
# A traceplot, if you like (and you saved fits)
# bayesplot::mcmc_trace(fit_1_1, regex_pars = "lambda")


## ----eval=TRUE, echo=FALSE, message=FALSE--------------------------------------------
# Sneaky way to get around the knitting/compilation problem
# biglist <- list()
# for(i in 1:4){
#   biglist[[i]] <- readRDS(paste0(here("Testing", "Theta1"), "/summary_df_for_scenario_",i,".rds"))
# }
# 
# sims_output <- do.call("bind_rows", biglist)


## ------------------------------------------------------------------------------------
visualize_parameter_group(sim_summary = sims_output, 
                          pars = "lambda", 
                          theta_scenario = 1, 
                          scenarios = 1:4, 
                          convergence_threshold = 1.1)


## ------------------------------------------------------------------------------------
# note the space between the indices for theta[2, 1]
visualize_single_parameter(sims_output, par = "theta[2, 1]", 
                           theta_scenario = 1, 
                           scenarios = 1:3, 
                           convergence_threshold = 1.2)

# New plotting functions, feedback welcome! (Also, not I'm not wed to either of these; 
# if you think it would be better to get rid of these functions and stick with what we 
# have, let me know! My reason for thinking of these is that this is likely the kind of
# really simple check that practitioners will be looking for.)

plot_coverage_vs_calls(
  sims_output, 
  validation_summary,
  regex_pars = "lambda",
  scenarios = 1:8, 
  theta_scenario = 1,
  convergence_threshold = 1.1
)

plot_bias_vs_calls(
  sims_output, 
  validation_summary, 
  pars = c("psi[1]", "psi[2]", "psi[3]"),
  scenarios = 1:8, 
  theta_scenario = 1,
  convergence_threshold = 1.1
)

# I THINK THESE ARE USEFUL, CAN YOU ADD ONE THAT PUTS INTERVAL WIDTH ON THE Y-AXIS?

# Added to my to-do list! 

## ----message=FALSE-------------------------------------------------------------------
psi <- c(0.633, 0.612, 0.849)
lambda <- c(5.934, 4.160, 14.25)

nspecies <- length(psi)
nsites <- 30
nvisits <- 4

Theta_FE <- t(apply(12*diag(nspecies) + 2, 1, function(x) nimble::rdirch(alpha = x)))

FE_data <- simulate_validatedData(
  n_datasets = 5, 
  validation_design = "FixedPercent",
  scenarios = c(0.05, .35, 0.65), # Note this is now a **vector** of possible scenarios. Also note the extremely small level of validation effort in the first entry!
  nsites = nsites, 
  nvisits = nvisits, 
  nspecies = nspecies,
  psi = psi, 
  lambda = lambda,
  theta = Theta_FE, 
  save_datasets = FALSE,
  save_masked_datasets = FALSE,
  directory = here("Testing", "FixedEffortExample")
)


## ------------------------------------------------------------------------------------
# Runtime: with the specified number of sites, visits, species, validation scenarios
# and parameter settings, this takes ~ 2:20 minutes per scenario, so around 6-7 minutes for
# this small test case. Plan accordingly! 

# RUNNING INTO ANOTHER ERROR HERE...
# Beginning scenario 1.
# 2024-09-30 12:21:22.319989
# |                                                  |   0%`summarise()` has grouped output by 'site'. You can override using the `.groups` argument.
# |==========                                        |  20%`summarise()` has grouped output by 'site'. You can override using the `.groups` argument.
# |====================                              |  40%`summarise()` has grouped output by 'site'. You can override using the `.groups` argument.
# |==============================                    |  60%`summarise()` has grouped output by 'site'. You can override using the `.groups` argument.
# |========================================          |  80%`summarise()` has grouped output by 'site'. You can override using the `.groups` argument.
# |==================================================| 100%
# Error in gzfile(file, mode) : cannot open the connection
# In addition: Warning message:
#   In gzfile(file, mode) :
#   cannot open compressed file '/Users/c84k467/Library/CloudStorage/OneDrive-MontanaStateUniversity/
#     BoxMigratedData/student-advising/oram-dissertation_work/ValidationExplorer/Testing/
      # FixedEffortExample/ThetaFE/summary_df_for_scenario_1.rds', 
# probable reason 'No such file or directory'

# This error is due to the directory ThetaFE not being set up ahead of time. New fix creates the directory for the user if it
# does not already exist. 

start <- Sys.time()
FE_model_fits <- run_sims(
  data_list = FE_data$masked_dfs,
  zeros_list = FE_data$zeros,
  theta_scenario_id = "FE",
  save_fits = FALSE,
  DGVs = list(lambda = lambda, psi = psi, theta = Theta_FE),
  save_individual_summaries_list = FALSE,
  directory = here("Testing", "FixedEffortExample")
)

Sys.time() - start


## ----echo=FALSE----------------------------------------------------------------------
# For faster knitting. Note that eval=FALSE in previous chunk. If you change 
# this setting and refit the model, your results may change! 
#saveRDS(FE_model_fits, paste0(here("Testing", "FixedEffortExample"), "/FE_model_fits.rds"))
# FE_model_fits <- readRDS(paste0(here("Testing", "FixedEffortExample"), "/FE_model_fits.rds"))


## ------------------------------------------------------------------------------------
visualize_parameter_group(FE_model_fits, pars = "lambda", theta_scenario = 1, scenarios = 1:3)


## ------------------------------------------------------------------------------------
visualize_parameter_group(FE_model_fits, pars = "psi", theta_scenario = 1, scenarios = 1:3)


## ------------------------------------------------------------------------------------
visualize_parameter_group(FE_model_fits, pars = "theta", theta_scenario = 1, scenarios = 1:3)


## ------------------------------------------------------------------------------------
summarize_n_validated(FE_data$masked_dfs, theta_scenario = "FE", scenario_names = as.character(1:3))

## ------------------------------------------------------------------------------------
plot_bias_vs_calls(
  FE_model_fits,
  summarize_n_validated(FE_data$masked_dfs, theta_scenario = "FE", scenario_names = as.character(1:3)), 
  theta_scenario = 1, 
  scenarios = 1:4, convergence_threshold = 1.1
)

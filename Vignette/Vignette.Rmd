---
title: "Vignette: Simulation studies with `ValidationExplorer` "
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    number_sections: yes
    extra_dependencies: ['float']
    keep_tex: no
    toc: no
    pandoc_args:
      - '--lua-filter=lua-filters/scholarly-metadata.lua'
      - '--lua-filter=lua-filters/author-info-blocks/author-info-blocks.lua'
header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
  - \usepackage{underscore}
bibliography: Vignette.bib
abstract: "Our vignette demonstrates the use of the `ValidationExplorer` package to conduct statistical simulation studies that can aid in the design phase of bioacoustic studies. In particular, our package facilitates explorion of the costs and inferential properties (e.g., coverage and interval widths) of alternative validation designs in the context of the count detection model framework. 
Our functions allow the user to specify a suite of candidate validation designs using either a stratified sampling procedure or a fixed-effort design type. An example of the former is provided in the manuscript entitled '`ValidationExplorer`: Streamlined simulations to provide bioacoustic study design guidance in the presence of misclassification', which was submitted to the Applications series of *Methods in Ecology and Evolution*. In this vignette, we provide further details not covered in the manuscript and an additional example of data simulation, model fitting, and visualization of simulation results when using a fixed-effort design type. Our demonstration here is intended to aid researchers and others to tailor a validation design that provides useful inference while also ensuring that the level of effort meets cost constraints. \\vfill"
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "H")
library(tidyverse)
library(nimble)
library(coda)
library(rstan)
library(parallel)
library(here)
library(kableExtra)
theme_set(theme_bw())

chunk_hook  <- knitr::knit_hooks$get("chunk") # single space code chunks
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- chunk_hook(x, options)
  paste0("\\linespread{1}\n", x, "\n\n\\linespread{1}")
})

devtools::load_all(path = "..")
```

------------------------------------------------------------------------

**Disclaimer:** This draft manuscript is distributed solely for the purposes of scientific peer review. Its content is deliberative and pre-decisional, so it must not be disclosed or released by reviewers. Because the manuscript has not yet been approved for publication by the U.S. Geological Survey (USGS), it does not represent any official USGS funding or policy. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.

\newpage

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```

\newpage

\doublespacing

# Introduction

Automated recording units (ARUs) provide one of the main data sources for many contemporary ecological studies that aim to provide inference about status and trends for assemblages of species [@loeb15]. As described in the main text, entitled, "`ValidationExplorer`: Streamlined simulations to provide bioacoustic study design guidance in the presence of misclassification," substantial practical interest lies in identifying cost-effective validation designs that will allow a study to obtain its measurable objectives. We believe that statistical simulation studies are a valuable tool for informing study design -- including the validation step -- prior to gathering ARU data, and it is our goal for `ValidationExplorer` to provide those tools in an approachable package.

This vignette aids exploration of possible validation designs within a count-detection framework using the `ValidationExplorer` package. As described in the main text, a validation design is composed of two parts: a random mechanism for selecting observations to be manually reviewed by experts ("validated"), and a percentage or proportion that controls the number of validated recordings. We refer to the random mechanism as the *design type* and the proportion as the level of validation effort (LOVE). In the following section we consider five possible LOVEs and show an example of a fixed effort design.

We emphasize that results from any simulation study -- including those produced using the `ValidationExplorer` package -- are conditional on the settings and assumptions of the study. In the case of the count-detection model framework that is implemented in `ValidationExplorer`, the assumptions are

\begin{enumerate}
  \item  The occurrence of species within a site are independent; the presence of one species carries no information about the presence or absence of another.
  \item For any one species, its occurrence at one location is independent of its occurrence at any other location (independence across sites).
  \item Visits to a site (i.e., detector nights) are independent. 
  \item Recordings within the same site-visit are independent.
  \item Each recording is assigned a single species autoID.
  \item Validation of one recording does not influence the probability that another will be validated.
  \item At sites where at least one species is present, the count of recordings generated by each species per night is a Poisson random variable.
  \item All species in the assemblage can co-occur and are capable of being confused (no structural zeros in the classification matrix). 
  \item The configuration of visits to sites is balanced. Note that this is not required to use our method but it is assumed in the simulation studies. For example, the data used by Oram et al. (in submission) had a varying number of detector nights at each site. To model these data, Oram et al. (in submission) made the additional assumption that the number of visits to a site is independent of the presence/absence and expected relative activity of species there. That is, locations where detectors were deployed for more nights were not more or less likely than other sites to be occupied by species of interest and were not more or less likely than other sites to have high levels of activity. 
  \item The fitted model is the same as the model that generated the data.
\end{enumerate}

\noindent Under these assumptions, we outline the step-by-step use of `ValidationExplorer` in the following section.

# Conducting a simulation study with a fixed effort design type \label{FEEx}

## What is a fixed effort design?

In this section, we assume a fixed-effort design type, under which $x\%$ of recordings obtained from each visit to a site are validated by experts. The level of validation effort is controlled by the value of $x.$ We begin the process of simulating under this design by defining the real-world objectives and constraints we anticipate.

## Step 0: Define measurable objectives and constraints \label{MO}

Recall from the main text that the first step -- before opening R and loading `ValidationExplorer` -- is to identify and write down the set of measurable objectives that the data will be used for. Suppose that, for this example, the measurable objectives and cost constraints are the same as those in Section 3 of the main text. That is, 

\begin{itemize}
  \item The measurable objective is to estimate relative activity parameters (denoted as $\lambda_k$) for each species with 95% coverage and expected posterior interval width less than 3 calls per night.
\item The monitoring program can pay their expert bat biologists to validate at most 4000 recordings. We make the assumption that all recordings are approximately the same cost to validate (i.e., rare species autoIDs are not necessarily more expensive or time consuming than extremely common ones). 
\end{itemize}

Suppose further that the species assemblage is the same as in the main text. That is, we have six species of interest that co-occur. The existing prior knowledge (perhaps from another study) about the relative activity rates and occurrence probabilities for each species are summarized in Table \@ref(tab:assemblage).

```{r assemblage, echo=FALSE}
psi <- c(0.6331, 0.6122, 0.8490, 0.6972, 0.2365, 0.7036)
lambda <- c(5.9347, 4.1603, 14.2532, 6.1985, 11.8649, 2.4050)

tibble(
  Species = c("Eptesicus fuscus (EPFU)", "Lasiurus cinereus (LACI)", 
              "Lasionycteris noctivagans (LANO)", "Myotis californicus (MYCA)", 
              "Myotis ciliolabrum (MYCI)", "Myotis evotis (MYEV)"),
  psi = round(psi,2),
  lambda = round(lambda,1)
) %>% 
  rename("$\\psi$" = "psi", "$\\lambda$" = "lambda") %>% 
  kbl(format = "latex", booktabs = TRUE, escape = FALSE, caption = "Prior knowledge of relative activity rates and occurrence probabilities for the six bat species of interest. These values will be used to simulate data. ")
```

## Step 1: Installing and loading required packages

Once measurable objectives and constraints are clearly defined, the next step is to load the required packages. For first time users, it may be necessary to install a number of dependencies, as shown by Table 1 in the main text. If you need to install a dependency or update the version, run the following, with `your_package_name_here` replaced by the name of the package:

```{r installDepends, eval=FALSE, echo=TRUE}
install.packages("your_package_name_here")
```

After installing the necessary packages, load these libraries by calling

```{r libDepends, eval=FALSE, echo=TRUE, message=FALSE}
library(tidyverse)
library(nimble)
library(coda)
library(rstan)
library(parallel)
library(here)
```

Finally, install and load `ValidationExplorer` by running

```{r installVE, echo=TRUE, eval=FALSE}
devtools::install_github(repo = "j-oram/ValidationExplorer")
library(ValidationExplorer)
```

With `ValidationExplorer` installed, users have access to all the functions outlined in the table of functions found in Section \@ref(BigTable).

## Step 2: Simulate data \label{dataSimulation}

The first step in a simulation study is to simulate data under each of the candidate validation designs, which is accomplished with the `simulate_validatedData` function in `ValidationExplorer`. We begin by assigning values for the number of sites, visits, and species, as well as the parameter values in Table \@ref(tab:assemblage), which are existing estimates obtained by @stratton22:

```{r pars}
# Set the number of sites, species and visits
nsites <- 30
nspecies <- 6
nvisits <- 4

psi <- c(0.6331, 0.6122, 0.8490, 0.6972, 0.2365, 0.7036)
lambda <- c(5.9347, 4.1603, 14.2532, 6.1985, 11.8649, 2.4050)
```

Note that by running multiple simulation studies with varying numbers of sites and balanced visits, `ValidationExplorer` allows users to investigate all elements of the study design prior to data collection. In addition to specifying the numnber of sites and visits and parameter values, `simulate_validatedData` requires that the user supply misclassification probabilities in the form of a matrix, subject to the constraint that rows in the matrix sum to one. An easy way to simulate a matrix of probabilities that meet these criteria is to leverage the `rdirch` function from the `nimble` package:

```{r Theta}
# Simulate a hypothetical confusion matrix 
set.seed(10092024) 
Theta <- t(apply(diag(29, nspecies) + 1, 1, function(x) {nimble::rdirch(alpha = x)}))
```

Note that the above definition of `Theta` places high values on the diagonal of the matrix, corresponding to a high probability of correct classification. To lower the diagonal values, change the specification of `diag(29, nspecies)` to a smaller value. For example:

```{r anotherTheta}
another_Theta <- t(apply(diag(5, nspecies) + 1, 1, function(x) {
    nimble::rdirch(alpha = x)
  }))
```

`another_Theta` has lower values on the diagonal, and greater off-diagonal values (i.e., higher probability of misclassification). If you have specific values you would like to use for the assumed classification probabilities (e.g., from an experiment), these can be supplied manually:

```{r manTheta}
manual_Theta <- matrix(c(0.9, 0.05, 0.01, 0.01, 0.02, 0.01, 
                       0.01, 0.7, 0.21, 0.05, 0.02, 0.01,  
                       0.01, 0.01, 0.95, 0.01, 0.01, 0.01,
                       0.05, 0.05, 0.03, 0.82, 0.04, 0.01,
                       0.01, 0.015,  0.005,  0.005, 0.95, 0.015,
                       0.003, 0.007, 0.1, 0.04, 0.06, 0.79), 
                       byrow = TRUE, nrow = 6)

print(manual_Theta)
```

If you define the classifier manually, make sure the rows sum to 1 by running

```{r ThetaCheck}
all(rowSums(manual_Theta) == 1) # want this to return TRUE

# If the above returns FALSE, see which one is not 1: 
rowSums(manual_Theta)
```

With the required inputs defined, we can simulate data:

```{r simData}
sim_data <- simulate_validatedData(
    n_datasets = 10, # For demonstration -- use 50+ for real simulation studies
    nsites = nsites, 
    nvisits = nvisits, 
    nspecies = nspecies, 
    design_type = "FixedPercent", 
    scenarios = c(0.05, 0.1, 0.15, 0.3),
    psi = psi, 
    lambda = lambda,
    theta = Theta, 
    save_datasets = FALSE, # default value is FALSE
    save_masked_datasets = FALSE, # default value is FALSE
    directory = here::here("Vignette", "Fixed_Effort")
)
```

Note that we specified the design type through the argument `design_type = "FixedPercent"`, with the possible scenarios defined as the vector `scenarios = c(0.05, 0.1, 0.15, 0.3)`. These two arguments specify the set of alternative validation designs we will compare in our simulation study. Under the first validation design, 5% of recordings from each visit to a site are validated, while in the second validation design 10% of recordings from each visit to a site are validated, and so on.

To understand the output from `simulate_validatedData`, we can investigate `sim_data`. The output is a list, containing three objects:

```{r}
names(sim_data)
```

We examine each of these objects below:

-   `full_datasets`: A list of length `n_datasets` with unmasked datasets. These are the datasets under complete validation so that every recording has an autoID and a true species label. We opted to not save these datasets to the working directory by setting `save_datasets = FALSE`. If we had specified `save_datasets = TRUE`, then these will be saved individually in `directory` as `dataset_n.rds`, where `n` is the dataset number. As an example of one element in `sim_data$full_datasets`, we examine the third simulated full dataset:

```{r fullDfs}
full_dfs <- sim_data$full_datasets
head(full_dfs[[3]]) # Dataset number 3 if all recordings were validated
```

Notice that in addition to the site, visit, true species and autoID (`id_spp`) columns, the parameter values (`lambda`, `psi`, and `theta`) are given for each true species-autoID combination. In addition, the occupancy state `z` for the true species is given and the `count` of calls at that site visit with a specific true species-autoID label combination. For example, at site 1, visit 1, there are four calls from species 1 that are assigned autoID 1, yielding 4 rows with `count = 4`. There is also one call from species 4 that was assigned a species 2 label with probability 0.02893559. Because this happened once, it is documented with `count = 1` and only occupies a single row. Next, we can see that there were 10 calls that were correctly identified as species 3; ten rows will have `true_spp = 3` and `autoID = 3`. Finally, the `Y.` column tells us how many observations were made from all species at that site visit; for visit 1 to site 1, the unique value is 24. That is, the 25th row of this dataset will contain the first observation from visit 2 to site 1.

-   `zeros`: A list of length `n_datasets` containing the true species-autoID combinations that were never observed at each site visit. These zero counts are necessary for the model to identify occurrence probabilities and relative activity rates. The `count` column, which, again, contains the count of each site-visit-true_spp-id_spp combination, is zero for all entries. For example, in dataset 3 (below), species 2 was not present at site 1, so it could not be classified as species 1 (first row). Additionally, species 3 was present at site 1, but it was never classified as species 1 on visit 1 (second row). If `save_datasets = TRUE`, the zeros for each dataset will also be saved in `directory` individually as `zeros_in_dataset_n.rds`, where `n` is the dataset number.

```{r zeros}
zeros <- sim_data$zeros

# The site-visit-true_spp-autoID combinations that were never observed in
# dataset 3. Notice that count = 0 for all rows! 
head(zeros[[3]]) 
```

-   `masked_dfs`: A nested list containing each dataset masked under each scenario. For example, `masked_dfs[[4]][[3]]` contains dataset 3, assuming that it was validated according to scenario 4 (30% of recordings randomly sampled from each site-visit for validation). If `save_masked_datasets = TRUE`, then each dataset/scenario combination is saved individually in `directory` as `dataset_n_masked_under_scenario_s.rds`, where `n` is the dataset number and `s` is the scenario number.

```{r mdfs}
masked_dfs <- sim_data$masked_dfs

# View dataset 3 subjected to the validation design in scenario 4: 
# randomly select and validate 30% of recordings from the first visit 
# to each site 
head(masked_dfs[[4]][[3]], 10)
```

The name of this nested list comes from the way that validation effort is simulated: recordings that are not selected for validation have their `true_spp` label masked with an `NA`. Notice that in the example output, all entries in the dataset are identical to the unmasked version output in `full_dfs` above, with the exception of the `true_spp` column. From this column we can see that calls 1-5 and 9 were not selected for validation (because `true_spp = NA`), while recordings 6-8 and 10 were (because the true species label is not marked as `NA`).

### Summarize validation effort \label{summarizeEffort}

For most simulations, it will be useful to summarize the number of recordings that are validated under a given validation design and scenario. This can be accomplished using the `summarize_n_validated` function:

```{r sumN}
summarize_n_validated(
  data_list = sim_data$masked_dfs, 
  theta_scenario = "1", 
  scenario_numbers = 1:4
)
```

We can see here that any of the validation designs considered in our simulations will remain well within budget.

## Step 3: MCMC tuning \label{tune}

Running a complete simulation study can be time consuming. In an effort to help users improve the efficiency of their simulations, we provide the `tune_mcmc` function, which outputs information about possible values for the warmup and total number of iterations required for the MCMC to reach approximate convergence. This function takes in a masked dataset and the corresponding zeros, fits a model to these data, and outputs an estimated run time for 10,000 iterations, as well as the estimated number of required warmup and total iterations. These are intended to assist tuning of the MCMC algorithm, which is done by the user in the following steps, which we walk through in greater detail below:

1.  Use `tune_mcmc` to fit a model with multiple long chains.

2.  Create trace plots for all model parameters.

3.  Examine effective sample sizes $n_\text{eff}$ and Gelman-Rubin statistics $\hat{R}$ for all parameters.

4.   Choose values for the number of iterations and warmup that are slightly larger than what is needed based on steps 1-3. This may help ensure that a greater number of model fits are available to inform simulation study results.

### Fit a model

As in the main text, we use a dataset from the scenario with the lowest number of validated recordings, as we expect the greatest number of iterations for this scenario. In our example, this is scenario 1 , in which an average of $\approx 218$ recordings are validated per dataset (Section \@ref(summarizeEffort)).

```{r tune, cache=TRUE}
scenario_number <- 1
dataset_number <- sample(1:length(masked_dfs[[scenario_number]]), 1)

tune_list <- tune_mcmc(
  dataset = sim_data$masked_dfs[[scenario_number]][[dataset_number]], 
  zeros = sim_data$zeros[[dataset_number]] 
)
```

The output from `tune_mcmc` is a list containing draws from the fitted model, the time required to fit the model with 10,000 draws, MCMC diagnostics and guesses for the number of iterations and warmup required to reliably fit a model. If the guessed values for total iterations and/or warmup are greater than 10,000 draws, an error is issued. We can see the names for each object by running the following block:

```{r}
names(tune_list)
```

The first element is the time required to fit a model with three chains of 10,000 iterations each:

```{r}
tune_list$max_iter_time
```

This may seem insignificant, but over the course of an entire simulations study with 5 scenarios $\times$ 50 datasets, that corresponds to around 8 hours of run time. Using fewer than 10,000 iterations will substantially reduce the time to run a simulation study.

### Create trace plots

To decide the number of iterations and warmup for use in a simulation study, we recommend beginning by creating trace plots, which show how the sampled values of a parameter evolve over the course of each Markov chain. To ensure the MCMC algorithm will characterize the posterior distribution well, we need to check that chains are stationary and mixing well, and that effective sample sizes are sufficiently large to characterize the posterior. Trace plots are especially useful for assessing the first of these. To create a trace plot using the bayesplot package [@bayesplot] for a single parameter, run the following: 

```{r}
# Load bayesplot package specifically designed for visualizing 
library(bayesplot)

# extract the fitted model from tune_mcmc output
fit <- tune_list$fit

# create traceplot
mcmc_trace(fit, pars = "lambda[5]")
```

We can see that far fewer than 10,000 iterations are likely required because the chains are mixing well after a few hundred iterations. This is shown by strongly overlapping chains, with no one chain appearing by itself in a region of the parameter space. Chains also appear to be stationary because they do not wander vertically substantially after the first few hundred iterations. 

We need to check that chains are stationary and mixing for all parameters. One way to accelerate visual inspection this is through the `regex_pars` argument, which shows relative activity parameters for all species as in the following three code blocks.

```{r}
mcmc_trace(fit, regex_pars = "lambda")
```

```{r}
mcmc_trace(fit, regex_pars = "psi")
```

```{r fig.width=10, fig.height=10}
# create a traceplot for all elements of the confusion matrix
mcmc_trace(fit, regex_pars = "theta")
```

In all of the trace plots for all of the parameters, chains appear to mix quickly, meaning that it is possible a far smaller number of iterations may be suitable for a simulation study. A good place to start for reducing the number of iterations is from the output given by `tune_mcmc`. In our case, we can see that a guess for the minimum number of iterations is `r tune_list$min_iter` with a warmup of `r tune_list$min_warmup`:

```{r}
tune_list$min_iter
tune_list$min_warmup
```

It is possible to use these values to zoom in on the trace plots by using the `window` argument: 

```{r}
mcmc_trace(fit, regex_pars = "lambda", window = c(0, tune_list$min_iter))
```

Once again, we see the three chains overlap strongly after around 500 iterations and sample around a horizontal line. 

After making traceplots and zooming in for all parameters (not all plots are shown here), it appears that the guessed warmup value of `r tune_list$min_warmup` output from `tune_mcmc` is a reasonable choice. Even so, we increase our iterations and warmup beyond these minimum values in Section \@ref(runSims) to avoid simulations failing due to convergence.

### Examine effective sample size and Gelman-Rubin statistics

As a final step, we examine the effective sample sizes (out of 10,000 draws) and $\hat{R}$ values. The effective sample size statistics `ess_bulk` and `ess_tail` are MCMC diagnostic statistics that summarizes the number of effectively independent draws from a parameter's posterior distribution the Markov chain contains. If a parameter has a large value in the `ess_bulk` column, then it is likely that inference based on sampled draws will characterize the center of the posterior distribution well. The `ess_tail` column, on the other hand, describes how much information is available about posterior tail probabilities. Once again, we want a large value for `ess_tail`, preferably `ess_tail`$\geq 250$. 

```{r}
tune_list$MCMC_diagnostics
```

For all parameters, the bulk and tail effective sample sizes are fairly large, meaning that even with fewer than 10,000 draws, we could expect $n_\text{eff} \geq 250,$ allowing us to characterize both the center and tails of the posteriors for all parameters with these draws. Furthermore, the $\hat{R}$ values for all parameters is near 1. In general, we want values of $\hat{R} \leq 1.1$ for chains to be considered converged. We can double check by recomputing these statistics on shortened chains: 

```{r}
# for each chain, extract iterations 1001:2500 for all parameters
shortened <- lapply(fit, function(x) x[1001:2500,])

# summarize the shortened chains and select the effective sample 
# size columns. mcmc_sum is an internal function used inside of `run_sims`, but
# we use it here to quickly obtain MCMC diagnostics for each parameter
mcmc_sum(shortened, truth = rep(0, ncol(shortened[[1]]))) %>%
  select(parameter, ess_bulk, ess_tail, Rhat)
```

These results appear satisfactory, with effective sample sizes in both the tail and bulk of the posterior distributions of more than 250 and $\hat{R}$ near 1. Based on the results of MCMC tuning, it appears that using an MCMC with at least `r tune_list$min_iter` iterations with  at least `r tune_list$min_warmup` discarded as warmup should to produce good results for our simulation study.

### Set iterations for simulation

Based on our findings in the MCMC tuning step, we set the number of iterations for simulation to be slightly higher to guard against convergence issues that preclude using a fitted model for inference: 

```{r}
# to be used in the following section 
iters_for_sims <- tune_list$min_iter + 1000
warmup_for_sims <- tune_list$min_warmup + 500
```

### A note about non-convergence while tuning

In some instances, we have run `tune_mcmc` with a dataset and received a series of error messages that convergence was not reached in under 10,000 iterations. If this persists after trying to fit several other datasets, we have several options:

1.  We could increase the number of iterations in the simulation study to be above 10,000 -- perhaps to 20,000 and settle for a longer run time of the simulation study.

2.  We could take this as a sign that the level of effort is insufficient to identify model parameters. In this case, this scenario should not be considered.

In our experience fitting these models, the second option seems to often be the case, and we encourage users to remove scenarios from consideration if models fit during the tuning step do not reach convergence within 10,000 iterations. It is possible that all validation scenarios -- including ones with very large levels of validation effort -- will lead to a lack of convergence if the number of sites and visits are very low, in which case the number of sites and visits should be re-evaluated. 

We emphasize that the results from `tune_mcmc` are from a single model fit; they are supplied only as guidelines, and we encourage users to increase the number of iterations above the minimum values output from `tune_mcmc`. While each model fit will take slightly longer with an increased number of total iterations, this approach may save time in the long run by avoiding the need to re-run simulations.

## Step 4: Fit models to simulated data \label{runSims}

With the simulated dataset and some informed choices about tuning of the MCMC, we use `run_sims` to run the simulations:

```{r run, cache=TRUE}
sims_output <- run_sims(
         data_list = sim_data$masked_dfs, 
         zeros_list = sim_data$zeros, 
         DGVs = list(lambda = lambda, psi = psi, theta = Theta), 
         theta_scenario_id = "FE", # for "fixed effort"
         parallel = TRUE, 
         niter = iters_for_sims, 
         nburn = warmup_for_sims,  
         thin = 1,
         save_fits = TRUE, 
         save_individual_summaries_list = FALSE, 
         directory = here::here("Vignette", "Fixed_Effort")
)
```

The output object, `sims_output` is a dataframe with summaries of the MCMC draws for each parameter estimated from each dataset under each validation scenario. Summaries include the posterior mean, standard deviation, Naive standard error, Time-series standard error, quantiles for 50% and 95% posterior intervals, median, and MCMC diagnostics (Gelman-Rubin statistic, and effective samples sizes in the tails and bulk of the distribution).

```{r}
str(sims_output)
```

Note that we fit all models in parallel to reduce simulation time; we encourage users to do the same. However, if you fit models with `parallel = FALSE` in `run_sims`, the console will display the messages NIMBLE prints as it compiles code for each model. Internally, we specify a custom distribution to compute the marginal probabilities for ambiguous autoID labels, and you will see a warning about overwriting a custom user-specified distribution if `parallel = FALSE`. These warnings can be safely ignored. 

## Step 5: Visualize simulations \label{viz}

Once models have been fit to all simulated datasets, you can visualize results using several functions. We recommend beginning with the most detailed functions, which are `visualize_parameter_group` and `visualize_single_parameter`. The plots output from these functions show many following features of the simulation study. These are

-   Facet grids: parameters (only for `visualize_parameter_group`)
-   X-axis: Manual verification scenario
-   y-axis: parameter values
-   Small grey error bars: 95% posterior interval for an individual model fit where all parameters were below `convergence_threshold`.
-   Colored error bars: average 95% posterior interval across all converged models under that scenario.
-   Color: Coverage, or the rate at which 95% posterior intervals contain the true data-generating parameter value.
-   Black points: the true value of the parameter
-   Red points: average posterior mean

The `visualize_parameter_group` function is useful for examining an entire set of parameters, such as all relative activity parameters. For example, we can visualize the inference for the relative activity parameters in the first three scenarios in our simulation study above by running the code below.

```{r vizGroup, eval=TRUE, fig.cap=" Example output from the `visualize_parameter_group` function. Results are faceted by parameter, with the x-axis indicating the validation scenario number, and the y-axis showing the data generating value (black point). Red points show average posterior means, small grey intervals show 95% posterior intervals from converged model fits and colored intervals are average 95% posterior intervals. Coverage is shown by color, with blue corresponding to low coverage."}
visualize_parameter_group(
  sim_summary = sims_output, 
  pars = "lambda", 
  theta_scenario = 1, 
  scenarios = 1:4,
  convergence_threshold = 1.05
)
```

The output shown in Figure \@ref(fig:vizGroup) indicates that under all validation scenarios we considered, the expected inference for relative activity rates of species 1-4 and 6 meets our measurable objectives: the posterior interval width is less than three for these species and there is minimal estimation error. However, note that under validation scenario 1 and 2, fewer of the models converged within `r iters_for_sims` iterations of the MCMC, which is visible from smaller number of grey intervals. This suggests that a higher level of validation effort could be beneficial. Additionally, average interval width is never below 3 for species 5; none of the designs considered here meet the measurable objective for this species. We can see this more clearly by examining output through alternative visualization functions.

A first step would be to use `visualize_single_parameter`, which takes the same arguments as the previous visualization function:

```{r vizOne, eval=TRUE, fig.cap="Example output from `visualize_single_parameter` is similar to that of `visualize_parameter_group`: the x-axis shows the validation scenario, and the y-axis shows the true value of the parameter (black point). Average posterior means are shown by red points, individual 95% posterior intervals from converged model fits are shown as small grey error bars and average 95% posterior intervals are shown by the larger colored error bars. The coverage is indicated by color."}
visualize_single_parameter(
  sims_output, par = "lambda[5]", 
  theta_scenario = 1, 
  scenarios = 1:4, 
  convergence_threshold = 1.05
)
```

The greater detail of `visualize_single_parameter` in Figure \@ref(fig:vizOne) underscores the difference in the number of converged models, shown by the number grey 95% posterior interval for each converged model fit. The visualization also helps clarify that while average interval width is near 3 in scenario 2, it is still acceptable for our measurable objectives. Coverage for most relative activity parameters is also near-nominal under this validation scenario, which requires that around 375 recordings be validated in an average dataset. This can also be seen using the `plot_width_vs_calls` function (Figure \@ref(fig:plotWidth)), which compares average interval widths (and the IQR for interval widths) across scenarios based on the number of recordings validated. An example is provided below:

```{r}
# obtain summary of validation effort as an object that can be used with 
# the summary plot functions plot_x_vs_calls
s <-  summarize_n_validated(
  data_list = sim_data$masked_dfs, 
  theta_scenario = "FE", # must not be NULL for the plot_X_vs_calls functions
  scenario_numbers = 1:4
)
```

```{r plotWidth, fig.cap="Example output from `plot_width_vs_calls`. The x-axis shows the number of validated recordings. Note that the level of effort increases with scenario number for the fixed-effort designs; this may not always be the case, for example, with a stratified-by-species design as in the main text. The y-axis shows the value of the average 95% posterior interval width (point). The middle 50% of interval widths for each parameter are shown by the error bars. Color indicates the parameter."}

plot_width_vs_calls(
  sim_summary = sims_output, 
  calls_summary = s, 
  regex_pars = "lambda", 
  theta_scenario = "FE", 
  scenarios = 2:5
) + # add horizontal line at target 95% CI width 
  geom_hline(yintercept = 3, linetype = "dotted") 
```

We provide analogous functions `plot_coverage_vs_calls` and `plot_bias_vs_calls` taking identical arguments that show how coverage and estimation error change with the number of calls validated. Note that in all of our visualization functions, if no fitted models have $\hat{R} \leq c$ for all parameters, where $c$ is the specified convergence threshold under a given scenario, then the scenario will not appear on the x-axis. In our example, we used $c = 1.1$. 

To ensure that there aren't substantial problems with estimation of other model parameters, we can create plots for $\psi$ and $\Theta$: 

```{r vizPsi, fig.cap="We encourage users to check inference for other model parameters, even if they are not explicitly of interest based on measurable objectives. Here, we examine $\\boldsymbol{\\psi}$. Note that coverage for several parameters is slightly low. For the purpose of exposition, we only fit 10 datasets and we expect that coverage would be near nominal levels if 50 or more datasets were used in simulation."}
visualize_parameter_group(
  sim_summary = sims_output, 
  pars = "psi", 
  theta_scenario = 1, 
  scenarios = 1:4,
  convergence_threshold = 1.05
)
```

```{r plotTheta, fig.width=10, fig.height=10, fig.cap="Checking inference for the elements of the classification matrix $\\Theta$. For most parameters (facets), validation scenario 2 appears to lead to minimal estimation error and near-nominal coverage. "}
visualize_parameter_group(
  sim_summary = sims_output, 
  pars = "theta", 
  theta_scenario = 1, 
  scenarios = 1:4,
  convergence_threshold = 1.05
)
```

The simulation results shown in Figure \@ref(fig:vizPsi) and \@ref(fig:plotTheta) don't immediately cause concern that inference for relative activity rates will be severely compromised for any of these validation designs. Some parameters have slightly low coverage, but we believe this is mostly due to the small number of datasets used in this example; users should increase the number of datasets to at least 50, which we expect would lead to coverage near nominal levels.

## Take-aways from the Fixed-Effort Simulations \label{takeaway-FE}

Based on Figures \@ref(fig:vizGroup) - \@ref(fig:plotWidth), we can rule out validation scenario 1, because the expected width of 95% posterior intervals is greater than the threshold value of 3. Because of this, we might choose validation scenario 2 as one possible fixed effort validation design. This fixed effort design assumes that 10% of recordings from each site-night are randomly selected for validation, leading to around 375 recordings being validated per dataset. In our simulations, posterior means for each $\lambda_k$ appear to be nearly unbiased, with near-nominal coverage and average posterior interval widths less than or equal to 3 for this scenario. However, it is possible that for a given dataset, the interval width for $\lambda_5$ will be larger than 3, regardless of the number of validated calls; error bars in Figure \@ref(fig:plotWidth) overlap or exceed the dotted line in all scenarios. 

In situations such as this one, a decision is required of the user. If a slightly larger interval width is acceptable, validating 10% percent of recordings from all site-visits is reasonable. If it is not, then the LOVE will need to be increased beyond that in scenario 4 (1021 recordings). The question is whether potentially tripling the number of validated recordings will be worth the benefit of ensuring the relative activity level for species 5 is less than 3. This is a question about measurable objectives that we leave to the practitioner and their study priorities.

\newpage

# Stratified-by-species example \label{stratEx}

In this section, we provide further details of the example in the main manuscript, which assumed a stratified-by-species design. In a stratified-by-species design, the random mechanism is a stratified random sample, with strata formed by the species labels assigned during automated classification (i.e., by autoID). In this type of design, the LOVE is the proportion of recordings selected to be validated for each species label.

We repeat the simulation study conducted in the main text in this section, providing greater detail for some of the steps. We assume the same measurable objectives and cost constraints as in Section \@ref(FEEx). 

## Simulate data \label{simBySpp}

Using the same values for `psi`, `lambda` and `Theta` as in Section \@ref(dataSimulation), we simulate data using `simulate_validatedData` with `design_type = "BySpecies"`. This will yield simulated data in the format described in Section \@ref(dataSimulation).

```{r message=FALSE}
sim_data <- simulate_validatedData(
  n_datasets = 10, 
  design_type = "BySpecies",
  scenarios = list(
    spp1 = 0.15, 
    spp2 = 0.15, 
    spp3 = c(0.15, .5, 1),
    spp4 = 0.1, 
    spp5 = c(0.5, 1),
    spp6 = 0.25
  ),
  nsites = nsites, 
  nspecies = nspecies, 
  nvisits = nvisits,
  psi = psi, 
  lambda = lambda, 
  theta = Theta, 
  directory = here::here("Vignette", "BySpecies")
)
```

Note that in contrast with Section \@ref(dataSimulation), `simulate_validatedData` expects the `scenarios` argument to be a list of proportions corresponding to the possible levels of effort for each species when specifying a stratified-by-species design.  Internally, `simulate_validatedData`  calls `base::expand.grid`, and considers all possible combinations of the various levels of effort for each species, meaning that the number of simulated scenarios grows extremely quickly. The biggest implication of having a larger number of simulation scenarios to consider is increased computation time. For example, in the previous code block we fixed the level of effort for species 1, 2, 4 and 6 at .15, .15, .1, and .25, respectively. There are three possible proportions to validate for species 3 and two possible levels of effort for species 5, yielding six possible scenarios, which are summarized in the additional output `sim_data$scenarios_df` that is available when `design_type = "BySpecies"` is specified: 

```{r}
names(sim_data)
sim_data$scenarios_df
```

If users would like to supply a tailored set of validation scenarios, this can be done through the `scen_expand` and `scen_df` arguments as follows: 

```{r}
# Define the scenarios dataframe. Each row is a scenario, each column is a spp
my_scenarios <- data.frame(
  spp1 = c(0.05, 0.05, 0.1), 
  spp2 = c(0.1, 0.2, 0.3),
  spp3 = c(0.2, 0.2, 0.25), 
  spp4 = c(0.4, 0.5, 0.6), 
  spp5 = rep(1, 3), 
  spp6 = rep(0.5, 3)
)

sim_data2 <- simulate_validatedData(
  n_datasets = 10, 
  design_type = "BySpecies",
  scen_expand = FALSE, 
  scen_df = my_scenarios,
  scenarios = NULL,
  nsites = nsites, 
  nspecies = nspecies, 
  nvisits = nvisits,
  psi = psi, 
  lambda = lambda, 
  theta = Theta, 
  directory = here::here("Vignette", "BySpecies")
)

sim_data2$scenarios_df # the same as my_scenarios above

```

In the remainder of this example, we use the `sim_data` object. 
We can combine the `scenarios_df` output with the output of `summarize_n_validated` to understand what the scenarios are and how many recordings are validated under each.

```{r}
summary1 <- sim_data$scenarios_df
summary2 <- summarize_n_validated(
  sim_data$masked_dfs, 
  scenario_numbers = 1:6,
  theta_scenario = "BySpecies"
)

call_sum <- left_join(summary1, summary2, by = "scenario")

call_sum
```

In the resulting dataframe, we see that scenario 1 has the lowest overall level of validation effort: species 1-3 have 15% of their recordings validated, species 4 has 10% validated, species 5 has 50%, and species 6 has 25%, yielding around 604 recordings validated in an average dataset. We use one dataset simulated under scenario 1 to tune the MCMC.

## Tune the MCMC

The MCMC tuning in this step is very similar to the procedures used in Section \@ref(tune). We begin by fitting a model to one dataset using the `tune_mcmc` function. 

```{r cache=TRUE}
i <- sample(1:length(sim_data$full_datasets), 1)

tune_list <- tune_mcmc(
  dataset = sim_data$masked_dfs[[1]][[i]],
  zeros = sim_data$zeros[[i]]
)
```

As in Section \@ref(tune), we visually inspect the chains for convergence using trace plots. We set trace plot windows to be wider than the suggested values from `tune_mcmc`, meaning a lower starting value and a higher ending value. Based on these plots, chains appear to be mixing well as no one chain stands out visually.  

```{r}
tune_list$min_warmup
tune_list$min_iter

# increase iters to visualize beyond the warmup and 
# iterations output from tune_mcmc 
fit <- tune_list$fit

mcmc_trace(fit, regex_pars = "lambda", window = c(0, tune_list$min_iter + 500))
```

```{r}
mcmc_trace(fit, regex_pars = "psi", window = c(0, tune_list$min_iter + 500))
```

```{r fig.width=10, fig.height=10}
mcmc_trace(fit, regex_pars = "theta", window = c(0, tune_list$min_iter + 500))
```

In all trace plots, it appears that MCMC chains have reached approximate convergence well before the warmup value of `r tune_list$min_warmup` draws output from `tune_mcmc`.

Next, we examine the effective sample sizes in the tails and bulk of the posteriors and the $\hat{R}$ values:

```{r}
tune_list$MCMC_diagnostics
```

For all parameters, the bulk and tail effective sample sizes are fairly large: if we slightly decrease the number of post-warmup draws, we could expect to characterize both the center and tails of the posteriors well.  Additionally, $\hat{R} \approx 1.00$ for all parameters, indicating good mixing for chains. As before, we check this suspicion by computing MCMC diagnostic statistics for truncated chains:

```{r}
# for each chain, extract iterations 501:2000 for all parameters
shortened <- lapply(fit, function(x) x[(tune_list$min_warmup):tune_list$min_iter + 1000,])

# summarize the shortened chains and select the effective sample 
# size columns
mcmc_sum(shortened, truth = rep(0, ncol(shortened[[1]]))) %>%
  select(parameter, ess_bulk, ess_tail)
```

The results appear satisfactory, with effective sample sizes of more than 250 in both the tail and bulk of the posterior distributions for each parameter. Based on the results of MCMC exploration, it appears that using an MCMC with `r tune_list$min_iter + 1000` iterations with `r tune_list$min_warmup` discarded as warmup is likely to produce good results for our simulation study.

## Fit models

```{r fitBySpecies, cache=TRUE }
sims_out <- run_sims(
  data_list = sim_data$masked_dfs,
  zeros_list = sim_data$zeros,
  DGVs = list(lambda = lambda, psi = psi, theta = Theta),
  theta_scenario_id = "BySpecies", 
  parallel = TRUE,
  niter = tune_list$min_iter + 1000, 
  nburn = tune_list$min_warmup + 500,
  thin = 1, 
  save_fits = FALSE,
  save_individual_summaries_list = FALSE,
  directory = here::here("Vignette", "BySpecies")
)
```
The output, `sims_out`, will have the same structure as described in Section \@ref(runSims).

## Visualize results \label{vizbyspp}

Recall that the measurable objectives of our study are to estimate the relative activity rates with estimation error less than 1 call per night and a 95% posterior interval width of less than 3 calls per night. Furthermore, we assume that the monitoring program can afford to validate 4000 calls in total. All of the possible validation designs shown in `sim_data$scenarios_df` are feasible given this budget. 

We begin with detailed plots of relative activity rates, occurrence probabilities, and classification probabilities. 

```{r vizLamBySpecies, fig.width=10, fig.height=10, fig.cap="Output from `visualize_parameter_group` under six possible stratified-by-species scenarios for relative activity rates. Parameters are shown in each facet and validation scenario number is on the x-axis. Small grey intervals are 95% posterior intervals for each parameter from fitted models that converged. Larger colored error bars are average 95% posterior intervals with the color indicating the coverage. Black dots are the true parameter value and red dots are average posterior means."}
visualize_parameter_group(
  sim_summary = sims_out, 
  pars = "lambda",
  theta_scenario = "BySpecies", 
  scenarios = 1:6, 
)
```

Based on the simulation results shown in Figure \@ref(fig:vizLamBySpecies), any of validation scenarios 1-6 is expected to produce a posterior mean estimate for each relative activity parameter that is near the true value. All models converged for all scenarios. Coverage varies by scenario for each parameter, but recall that we only fit models to 10 datasets. 

```{r vizPsiBySpecies, fig.width=10, fig.height=10, fig.cap="Output from `visualize_parameter_group` under six possible stratified-by-species scenarios for occurrence probabilities. Parameters are shown in each facet and validation scenario number is on the x-axis. Small grey intervals are 95% posterior intervals for each parameter from fitted models that converged. Larger colored error bars are average 95% posterior intervals with the color indicating the coverage. Black dots are the true parameter value and red dots are average posterior means."}

visualize_parameter_group(
  sim_summary = sims_out, 
  pars = "psi",
  theta_scenario = "BySpecies", 
  scenarios = 1:6, 
)
```

The simulation results for occurrence probabilities show a small amount estimation error for all occurrence probabilities, with the size and direction varying depending on the species. For species with larger estimation error, coverage is also slightly low. However, since we are most concerned with relative activity, the goal is to check results for each $\psi_k$ for severe estimation error and/or lack of coverage, which are not shown in Figure \@ref(fig:vizPsiBySpecies). Similarly, we do not observe alarming results in Figure \@ref(fig:vizThetaBySpecies). 

```{r vizThetaBySpecies, fig.width=10, fig.height=10, fig.cap="Output from `visualize_parameter_group` under six possible stratified-by-species scenarios for classification probabilities. Parameters are shown in each facet and validation scenario number is on the x-axis. Small grey intervals are 95% posterior intervals for each parameter from fitted models that converged. Larger colored error bars are average 95% posterior intervals with the color indicating the coverage. Black dots are the true parameter value and red dots are average posterior means."}

visualize_parameter_group(
  sim_summary = sims_out, 
  pars = "theta",
  theta_scenario = "BySpecies", 
  scenarios = 1:6, 
)
```

One measurable objective is for 95% posterior interval widths to be less than 3 calls per night for each species' relative activity rate. As in Section \@ref(viz), the `plot_width_vs_calls` function provides Figure \@ref(fig:widthVscalls), which addresses this measurable objective directly. 

```{r widthVscalls, fig.width=10, fig.height=10, fig.cap="Plots of 95% posterior interval width vs the number of calls validated."}
plot_width_vs_calls(
  sim_summary = sims_out, 
  calls_summary = call_sum, 
  regex_pars = "lambda",
  theta_scenario = unique(sims_out$theta_scenario),
  scenarios = 1:6
)
```

Once again as in Section \@ref(viz), the widest posterior interval width is for species 5. All validation scenarios have average 95% posterior interval widths near or slightly below 3 calls per night, but the error bars indicate an interval width greater than 3 is possible for any validation scenario. Scenario 6, in which 1785 recordings are validated, has the narrowest expected interval width. 

## Take-aways \label{takeaway-bySpp}

The results in Section \@ref(vizbyspp), imply that, of the stratified-by-species validation designs considered, scenario 6 offered the best results with respect to the measurable objectives we outlined in Section \@ref(MO). In this scenario, the six species in our assemblage receive 15%, 15%, 100%, 10%, 100%, and 25% of their recordings validated, respectively. 

To keep the number of scenarios in this example small, we fixed the LOVE for several species. However, if the measurable objectives specified the desired accuracy and precision for estimates of occurrence probability, it may be desirable to consider alternative validation scenarios. For example, we consistently underestimated the occurrence probability for species 6 (Figure \@ref(fig:vizPsiBySpecies)), and additional simulations under validation designs that varied the level of effort for species 6 would be warranted if measurable objectives related to occurrence probability for this species. 

# Conclusion 

We have demonstrated the use of the `ValidationExplorer` package when the objective is to compare the merits of four competing LOVEs for fixed-effort and stratified-by-species designs. Specifically, in the fixed-effort design we considered validating a random sample of 5%, 10%, 15% or 30% of the recordings obtained during each visit to each site. In the stratified-by-species designs, we considered a suite of validation designs, fixing effort for species 1,2, 4, and 6 at less than or equal to 25%, and varied the level of effort for species 3 and 5. The exact simulations considered are summarized in the `call_sum` object in Section \@ref(simBySpp). The measurable objectives outlined in Section \@ref(MO) were to estimate the relative activity parameters for each species, with near-nominal coverage and width less than 3 calls per night for 95% posterior intervals. Results, and their implications for these measurable objectives were summarized in Sections \@ref(takeaway-FE) and \@ref(takeaway-bySpp). Practitioners who would like to inform study design prior to data collection can repeat multiple simulation studies to see how inferences change with the number of sites and balanced visits, in addition to the validation design.


\newpage

# Table of Functions \label{BigTable}

\singlespacing
\begin{longtable}{p{0.27\linewidth}p{0.32\linewidth}p{0.33\linewidth}}
    \toprule
        Function & Argument & Description   \\
        \midrule
        
        \texttt{simulate\textunderscore validatedData} &  \texttt{n\textunderscore datasets} & The number of datasets to be simulated \\ 
        \addlinespace
        & \texttt{design\textunderscore type} & The type of validation design. Must be one of \texttt{"BySpecies"} or \texttt{"FixedPercent"} \\ 
        \addlinespace
        & \texttt{scenarios} & The possible levels of effort. If \texttt{design\textunderscore type = "BySpecies"}, this is provided as a list with vector-valued entries containing the possible percentages to validate for each species. If \texttt{design\textunderscore type = "FixedPercent"}, this argument is a vector of possible percentages. \\ 
        \addlinespace
        & \texttt{n\textunderscore sites, n\textunderscore visits, n\textunderscore species} & The number of sites, visits and species in the assemblage \\ 
        \addlinespace
        & \texttt{psi, lambda, theta},  & Vectors of length \texttt{nspecies} containing the the parameter values for each species \\ 
        \addlinespace
        & \texttt{directory} & The working directory where datasets are to be saved if the following arguments are set to TRUE \\
        \addlinespace
        & \texttt{save\textunderscore datasets, save\textunderscore masked_datasets} & Logicals indicating whether to save each type of dataset \\
        \addlinespace

        \hline
        \texttt{summarize\textunderscore n\textunderscore validated} & \texttt{data\textunderscore list} & A list of simulated (masked) datasets output from \texttt{simulate\textunderscore validatedData} \\ 
        \addlinespace 
        & \texttt{zeros\textunderscore list} & A list of true species/autoID combinations that were never observed at each site-visit. \\ 
        \addlinespace
        & \texttt{theta\textunderscore scenario} & An optional character string identifying the classifier scenario. If output from \texttt{summarize\textunderscore n\textunderscore validated} is to be used with any of the \texttt{plot\textunderscore X\textunderscore vs\textunderscore calls} functions described below, this string must match the \texttt{theta\textunderscore scenario\textunderscore id} argument supplied to \texttt{run\textunderscore sims}. \\
        \addlinespace

        \hline
        \texttt{run\textunderscore sims} & \texttt{data\textunderscore list} & A nested list of masked datasets in the format output from \texttt{simualate\textunderscore validatedData}. The first layer of the list corresponds to scenarios, with each entry containing a list of \texttt{n\textunderscore datasets} validated according to the scenario. \\ 
        \addlinespace
        & \texttt{zeros\textunderscore list} & A list of length \texttt{n\textunderscore datasets} containing the site-visit-true-autoID combinations that were never observed. \\ 
        \addlinespace 
        & \texttt{DGVs} & A named list of the true data-generating values with entries \texttt{"psi", "lambda"} and \texttt{"theta"}. \\ 
        \addlinespace
        & \texttt{theta\textunderscore scenario\textunderscore id} & An ID to show which classifier the scenario is under. This is provided as a convenience for the user if multiple simulation studies are to be conducted.\\
        \addlinespace
        & \texttt{parallel} &  A logical indicating whether MCMC sampling should be fit in parallel (default setting is TRUE). If you have many datasets and many scenarios, we recommend this setting. \\
        \addlinespace
        & \texttt{n\textunderscore iter, nburn, thin} &  The number of iterations, warmup and thinning interval for each chain in the MCMC. Default values are 2000, 1000, and 1, respectively. \\ 
        \addlinespace
        & \texttt{save\textunderscore fits} &  A logical denoting whether or not to save the draws from individual fitted models. If TRUE, you must have the file structure described in Step 2. Fits will be saved as RDS objects that can be read in later. Default value is FALSE.\\
        \addlinespace
        & \texttt{save\textunderscore individual\textunderscore summaries\textunderscore list} & A logical indicating whether to save individual summary lists that are output after each validation scenario. Default value is FALSE.\\ 
        \addlinespace
        & \texttt{directory} &  Where to save fits and summaries. Required if \texttt{save\textunderscore fits\ =\ TRUE} or \texttt{save\textunderscore individual\textunderscore summaries\textunderscore list\ =\ TRUE}. Default value is the current working directory given by \texttt{here::here()} \\ 
        \addlinespace 

        \hline
    
        \texttt{visualize\textunderscore parameter\textunderscore group} & \texttt{sim\textunderscore summary} & A dataframe in the format of the summaries output by \texttt{run\textunderscore sims}. Column names must match those of the \texttt{run\textunderscore sims} output. \\ 
        \addlinespace 
        & \texttt{pars} & The name of the parameter ``group'' to be visualized (e.g, \texttt{"psi"}, \texttt{"lambda"} or \texttt{"theta"}). \\ 
        \addlinespace 
        & \texttt{theta\textunderscore scenario}&  The \(\Theta\) classifier ID. \\ 
        \addlinespace 
        & \texttt{scenarios} &  Which scenarios to visualize? \\ 
        \addlinespace 
        & \texttt{convergence\textunderscore threshold}& What value should $\hat{R}$ be below to be considered ``converged''? Default value is 1.1. This value matters because only model fits where all parameter values are below the \texttt{convergence\textunderscore threshold} are used for visualization. \\ 
        \addlinespace 
        
        \hline
        \texttt{visualize\textunderscore single\textunderscore parameter} & & Arguments are identical to  \texttt{visualize\textunderscore parameter\textunderscore group} \\ 
        \addlinespace
        
        \hline 
        
        \texttt{plot_bias_vs_calls}, \texttt{plot_coverage_vs_calls}, \texttt{plot_width_vs_calls} & \texttt{sims_summary} & The summary output in the format from \texttt{run\textunderscore sims} \\ 
        \addlinespace
        & \texttt{calls\textunderscore summary} & A summary of the number of calls validated per scenario. Expected format is that of output from \texttt{summarize\textunderscore n\textunderscore validated}. \\ 
        \addlinespace
        & \texttt{pars} & The parameters to visualize. \\ 
        \addlinespace
        & \texttt{regex\textunderscore pars} & A group of parameters to visualize. One of \texttt{"psi"}, \texttt{"lambda"} or \texttt{"theta"}. \\ 
        \addlinespace
        & \texttt{theta\textunderscore scenario} & The classifier scenario ID. \\
        \addlinespace
        & \texttt{scenarios} & The scenarios to be compared. \\
        \addlinespace
        & \texttt{convergence\textunderscore threshold} & At what value is an MCMC algorithm considered ``converged"? \\ 
        \addlinespace
        \bottomrule
    \caption{Argument descriptions for each function in the \texttt{ValidationExplorer} software. My hope is that in the eventual final paper, this will be a scrollable html table}
    \label{tab:my_label}
\end{longtable}

# References

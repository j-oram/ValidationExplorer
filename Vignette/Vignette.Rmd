---
title: "Vignette: Simulation studies with `ValidationExplorer` under a fixed-effort design type"
author:
  - name: Jacob Oram
    email: jacoboram@montana.edu
    institute: msu
    correspondence: false
  - name: Katharine Banner
    email: katharine.banner@montana.edu
    institute: msu
    correspondence: false
  - name: Christian Stratton
    email: cstratton@middlebur.edu
    institute: middlebury
    correspondence: false
  - name: Kathryn M. Irvine
    email: kirvine@usgs.gov
    institute: norock
    correspondence: true
institute:
    - msu: Department of Mathematical Sciences, Montana State University, Bozeman, MT, USA
    - middlebury: Department of Mathematics and Statistics, Middlebury College, Middlebury, VT, USA
    - norock: U.S. Geological Survey, Northern Rocky Mountain Science Center, Bozeman, MT, USA
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    number_sections: yes
    keep_tex: no
    toc: no
    pandoc_args:
      - '--lua-filter=lua-filters/scholarly-metadata.lua'
      - '--lua-filter=lua-filters/author-info-blocks/author-info-blocks.lua'
header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
bibliography: Vignette.bib
abstract: "Our vignette demonstrates the use of the `ValidationExplorer` package, which was designed to facilitate statistical simulation studies that explore the costs and inferential benefits (e.g., near nominal coverage and minimal estimation error) of alternative validation designs. 
Our functions allow the user to specify a suite of candidate validation designs using either a stratified sampling procedure or a fixed-effort design type. An example of the former is provided in the manuscript entitled 'ValidationExplorer`: Streamlined simulations to aid informed management decisions using bioacoustic data in the presence of misclassification', which was submitted to the Applications series of *Methods in Ecology and Evolution*. In this vignette, we provide an additional example of data simulation, model fitting, and visualization of simulation results when using a fixed-effort design type. Our demonstration here is intended to aid researchers and others to tailor a validation design that provides useful inference while also ensuring that the level of effort meets cost constraints. \\vfill"
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nimble)
library(coda)
library(rstan)
library(parallel)
library(here)
library(kableExtra)
theme_set(theme_bw())

chunk_hook  <- knitr::knit_hooks$get("chunk") # single space code chunks
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- chunk_hook(x, options)
  paste0("\\linespread{1}\n", x, "\n\n\\linespread{1}")
})
```

------------------------------------------------------------------------

**Disclaimer:** This draft manuscript is distributed solely for the purposes of scientific peer review. Its content is deliberative and pre-decisional, so it must not be disclosed or released by reviewers. Because the manuscript has not yet been approved for publication by the U.S. Geological Survey (USGS), it does not represent any official USGS funding or policy. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.

\newpage

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

\doublespacing

# Introduction

Automated recording units (ARUs) provide one of the main data sources for many contemporary monitoring programs that aim to provide inference about status and trends for assemblages of species [@loeb15]. As described in "`ValidationExplorer`: Streamlined simulations to aid informed management decisions using bioacoustic data in the presence of misclassification" (hereafter, "the main text"), substantial practical interest lies in identifying cost-effective validation designs that will allow a monitoring program to obtain its measurable objectives. We believe that statistical simulation studies are a valuable tool for evaluating the relative merits of candidate validation designs prior to gathering and validating ARU data, and it is our goal for `ValidationExplorer` to provide those tools. 

This vignette provides detailed demonstrations of the `ValidationExplorer` package under a fixed-effort design. As described in the main text, this design assumes that $x\%$ of recordings obtained from the first visit to a site are validated by experts. The level of validation effort (LOVE), or sample size, of the validation design is controlled by the value of $x.$ In the following section we consider four possible LOVEs. 


# Conducting a simulation study with a fixed effort design type

Recall from the main text that the first step -- before opening R and loading `ValidationExplorer` --  is to identify and write down the set of measurable objectives that the data will be used for. Suppose that, for this example, the assumed species assemblage and measurable objectives are the same as those in the example provided in the main text: 




## Step 1: Installing and loading required packages

Our package has a number of dependencies 

```{r eval=FALSE, echo=TRUE}
install.packages("your_package_name_here")
```

After installing the necessary packages, load these libraries by calling

```{r eval=FALSE, echo=TRUE, message=FALSE}
library(tidyverse)
library(nimble)
library(coda)
library(rstan)
library(parallel)
library(here)
# library(ValidationExplorer)
```


## Step 3: Simulate data {dataSimulation}

```{r}
# Set the number of sites,species and visits
nsites <- 30
nspecies <- 6
nvisits <- 4

# Simulate a hypothetical confusion matrix 
set.seed(10092024) 
Theta <- t(apply(diag(29, nspecies) + 1, 1, function(x) {nimble::rdirch(alpha = x)}))
```

```{r}
psi <- c(0.6331, 0.6122, 0.8490, 0.6972, 0.2365, 0.7036)
lambda <- c(5.9347, 4.1603, 14.2532, 6.1985, 11.8649, 2.4050)
    
sim_data <- simulate_validatedData(
    n_datasets = n_datasets,
    nsites = nsites, 
    nvisits = nvisits, 
    nspecies = nspecies, 
    design_type = "FixedPercent", 
    scenarios = c(0.05, 0.1, 0.15, 0.3, 0.5),
    psi = psi, 
    lambda = lambda,
    theta = Theta
)
```

The data simulation function also requires that we define the classifier and validation scenarios to be used in the simulations. Here, the classifier is encoded as a matrix with rows corresponding to the true species that generated the call, and columns corresponding to the assigned autoID label. Thus the $i,j^\text{th}$ entry of the matrix (denoted $\theta_{ij}$) is the probability that species $i$ is classified as species $j.$ In `test_theta1` below, the probability that species 2 is classified as species 1 is 0.1, and the probability that species 3 is correctly classified as species 3 is 0.95.

To define a classifier, you can manually enter a matrix:

```{r}
test_theta1 <- matrix(c(0.9, 0.05, 0.05,
                       0.1, 0.85, 0.05, 
                       0.02, 0.03, 0.95), byrow = TRUE, nrow = 3)

test_theta1
```

This can also be accomplished by using the `rdirch` function from NIMBLE:

```{r}
test_theta2 <- t(apply(18*diag(nspecies) + 2, 1, function(x) nimble::rdirch(alpha = x)))
test_theta2
```

However you choose to generate the $\Theta$ matrix, make sure that the rows sum to 1:

```{r}
rowSums(test_theta1)
rowSums(test_theta2)
```

The next input that needs to be defined for the simulation is the validation efforts for each species label. These are to be stored in a dataframe with each row corresponding to a validation design you would like to investigate. In a simple scenario with the three species above and two levels of effort for each, we could generate an appropriate dataframe by running `expand.grid`. This yields 8 distinct validation scenarios:

```{r}
val_scenarios <- expand.grid(spp1 = c(.75, .5), spp2 = c(.25, .5), spp3 = c(.25, .75))
val_scenarios
nrow(val_scenarios)
```

With the appropriate inputs defined, we can simulate data. Note that in this example we opt to save both the simulated datasets with all true species labels retained (`save_datasets = TRUE`), as well as the simulated datasets with all true species labels masked except for those that were validated according to the validation scenario (`save_masked_datasets = TRUE`). These datasets are saved in the Testing folder of the current working directory. Note that if you specify the directory using `here::here()`, as we have, the subfolder must have a slash in front of it (e.g., `"/Testing"`). Note that every dataset under every validation set is saved separately, meaning that there will be $2 \times$ `n_datasets` + `n_datasets` $\times$ `nrow(scenarios)` objects saved in your directory!

```{r message=FALSE}
fake_data <- simulate_validatedData(
  n_datasets = 5, 
  validation_design = "BySpecies",
  scenarios = val_scenarios, 
  nsites = nsites, 
  nvisits = nvisits, 
  nspecies = nspecies,
  psi = psi, 
  lambda = lambda,
  theta = test_theta2, 
  save_datasets = TRUE,
  save_masked_datasets = TRUE,
  directory = paste0(here::here("Testing"))
)
```

To see what is available, we can investigate `fake_data`. The output is a list, containing three objects:

-   `full_datasets`: A list of length `n_datasets` with unmasked datasets (i.e., full validation of all recordings). If `save_datasets = TRUE`, then these will be saved individually in `directory` as dataset_n.rds, where n is the dataset number.
-   `zeros`: A list of length `n_datasets` containing all of the site-visits where no recordings of a certain classification were observed. For example, if, in dataset 10, there were no calls from species 1 that were classified as 3 on visit 4 to site 156, then the 10th entry of this list would contain a dataset with a row corresponding to site = 156, visit = 4, true_spp = 1, id_spp = 3, with count = 0. These zeros are necessary for housekeeping in the model-fitting process. If `save_datasets = TRUE`, the zeros for each dataset will be saved in `directory` individually as site_visits_without_calls_in_dataset_n.rds, where n is the dataset number.
-   `masked_dfs`: A nested list containing each dataset masked under each scenario. For example, `masked_dfs[[9]][[27]]` contains dataset 27, assuming validation scenario 9. If `save_masked_datasets = TRUE`, then each dataset/scenario combination is saved individually in `directory` as dataset_n_masked_under_scenario_s.rds, where n is the dataset number and s is the scenario number.

Examples of each are given below:

```{r}
full_dfs <- fake_data$full_datasets
head(full_dfs[[1]])
```

```{r}
site_visits_without_calls <- fake_data$zeros
head(site_visits_without_calls[[1]])
```

```{r}
masked_dfs <- fake_data$masked_dfs

# View dataset 3 with scenario 7 validation effort.
head(masked_dfs[[7]][[3]])
```

For most simulations, it will be useful to summarize the number of recordings that are validated under a given validation design and scenario. This can be accomplished using the `summarize_n_validated` function, which outputs a vector containing the average number of recordings validated in a dataset under each scenario. Note that in our example, there are 8 scenarios (each contained in a row of the `val_scenarios` dataframe), meaning that the output is of length 8:

```{r}
source("../Data Simulation/summarize_n_validated.R")
summarize_n_validated(data_list = fake_data$masked_dfs)
```

## Step 4: Fit the model

With the simulated data stored in the global environment, we can now fit models to the simulated datasets using NIMBLE via the wrapper function `run_sims`. This function requires specification of the following arguments:

-   `data_list`: The list of masked datasets output from `simulate_validatedData`.
-   `zeros_list`: The list of site-visit-true-autoID combinations that were never observed. This is the `zeros` object output from `simulate_validatedData`.
-   `theta_scenario_id`: An ID to show which classifier the scenario is under. This must match the name of the directory `.../ThetaID` if you want to save model fits and summaries.
-   `parallel`: A logical indicating whether MCMC sampling should be fit in parallel (default setting is TRUE). If you have many datasets and many scenarios, we recommend this setting.
-   `initialize_lambda_near_naive_val`: A logical indicating whether to initialize chains for $\lambda_k$ near the naive average count of recordings from each species.
-   `n_iter`: The number of iterations for each chain in the MCMC. Default value is 2000.
-   `nburn`: The number of warmup iterations before MCMC draws are retained. By default, half of `n_iter` draws are discarded as warmup.
-   `thin`: Thinning of the MCMC chains.
-   `save_fits`: A logical denoting whether or not to save the draws from individual fitted models. If TRUE, you must have the file structure described in Step 2. Fits will be saved as RDS objects that can be read in later. Default value is FALSE.\
-   `save_individual_summaries_list`: A logical indicating whether to save individual summary lists that are output after each validation scenario. Defalut value is FALSE
-   `directory`: where saved fits and summaries should be saved. Required if `save_fits = TRUE` or `save_individual_summaries_list = TRUE`. Default value is the current working directory given by `here::here()`.

An example of using this function with the simulated data from step 3 is given below:

```{r eval = FALSE}
source("../Model Fitting & Simulation/run_sims.R")
# takes about 40 minutes with 10k draws for 5 datasets
sims_output <- run_sims(
         data_list = fake_data$masked_dfs, 
         zeros_list = fake_data$zeros, 
         DGVs = list(lambda = lambda, psi = psi, theta = test_theta2), 
         theta_scenario_id = 1, parallel = TRUE, 
         niter = 2000, thin = 1,
         save_fits = TRUE, 
         save_individual_summaries_list = FALSE, 
         directory = here("Testing"))
```

## Step 5: Assess MCMC convergence

If you selected `save_fits = TRUE` in the `run_sims` function, then individual model fits will be available in `your/directory/fits`. You can use these, together with the Bayesplot package (run `install.packages("bayesplot")` and then `library(bayesplot)` if you do not have this package installed), to visualize model fits through a wide variety of plots. For example, if you would like to see a density plot for species 1's relative activity level in the first dataset under the first validation scenario, you would read in the fit and visualize using `mcmc_dens_overlay`:

```{r}
# read in fit object
fit_1_1 <- readRDS("../Testing/Theta1/fits/fit_1_1.rds")

# visualize using bayesplot
bayesplot::mcmc_dens_overlay(fit_1_1, pars = "lambda[1]")
```

To see a traceplot for all $\lambda$ parameters, run the following:

```{r}
bayesplot::mcmc_trace(fit_1_1, regex_pars = "lambda")
```

The Bayesplot package has many other visualizations that are available. See their [website](http://mc-stan.org/bayesplot/) for more examples and details (<http://mc-stan.org/bayesplot/>).

The density plots show substantial overlap of the three bell-shaped chains, indicating that the MCMC algorithm has converged. This is further supported by visual inspection of traceplots, which show good mixing. In a simulation study, visual inspection of all traceplots for all parameters in each fitted model is infeasible. Instead, we recommend checking summary tables to make sure that a dataset is included only if all parameters in the model fit to that dataset have Gelman-Rubin statistics $\hat{R} \leq 1.1.$ This can be achieved using functions from the dplyr package, where we can see that all parameters were below 1.1, in all five datasets under scenarios 1-4 (which are the only ones considered here).

```{r eval=TRUE, echo=FALSE, message=FALSE}
# Sneaky way to get around the knitting/compilation problem
biglist <- list()
for(i in 1:4){
  biglist[[i]] <- readRDS(paste0(here("Testing", "Theta1"), "/summary_df_for_scenario_",i,".rds"))
}

sims_output <- do.call("bind_rows", biglist)
```

```{r}
sims_output %>% 
  group_by(theta_scenario, scenario, dataset) %>% 
  mutate(all_pars_converged = ifelse(unique(converge) == 1, 1, 0)) %>% 
  filter(all_pars_converged == 1) %>% 
  ungroup () %>% 
  select(theta_scenario, scenario, dataset) %>% 
  distinct()
```

Note that the visualization functions described in the next section automatically filter to only include model results if the MCMC algorithm shows evidence of convergence.

## Step 6: Visualize simulations

Once the simulation study is complete, you can visualize the results using two functions, `visualize_parameter_group` and `visualize_single_parameter`. These functions ensure that only converged models are included in the visualization. `visualize_parameter_group` is useful for examining an entire set of parameters, such as all relative activity parameters. The set of expected inputs for `visualize_parameter_group` are:

-   `sim_summary`: A dataframe in the format of the summaries output by `run_sims`. Column names must match those of the `run_sims` output.
-   `pars`: The name of the parameter "group" to be visualized (e.g, "psi", "lambda" or "theta").
-   `theta_scenario`: The $\Theta$ classifier ID.
-   `scenarios`: Which scenarios to visualize?
-   `convergence_threshold`: What value should $\hat{R}$ be below to be considered "converged"? Default value is 1.1. This value matters because only model fits where all parameter values are below the `convergence_threshold` are used for visualization.

We can visualize the inference for the relative activity parameters in the first three scenarios in our simulation study above by running the code below. Note that we have set `convergence_threshold` to be unrealistically high for illustration purposes. Typically, the default value of 1.1 is as high of an $\hat{R}$ value as possible for the MCMC chains to be considered "converged".

```{r}
source("../Summary Figures/visualize_sims.R")
visualize_parameter_group(sim_summary = sims_output, 
                          pars = "lambda", 
                          theta_scenario = 1, 
                          scenarios = 1:4, 
                          convergence_threshold = 1.1)
```

The features of the plot are as follows:

-   Facet grids: parameters
-   X-axis: Manual verification scenario
-   y-axis: parameter values
-   Small grey error bars: 95% posterior interval for an individual model fit where all parameters were below `convergence_threshold`.
-   Colored error bars: average 95% posterior interval across all converged models under that scenario.
-   Color: Coverage, or the rate at which 95% posterior intervals contain the true data-generating parameter value.
-   Black dots: the true value of the parameter
-   Red dots: average posterior mean

If you would like to visualize a single parameter, use `visualize_single_parameter`, which takes the same arguments as the previous visualization function:

```{r}
# note the space between the indices for theta[2, 1]
visualize_single_parameter(sims_output, par = "theta[2, 1]", 
                           theta_scenario = 1, 
                           scenarios = 1:3, 
                           convergence_threshold = 1.2)
```

Note that the scale of the y-axis is free to change from one visualization to the next. Additionally, if no datasets show evidence of convergence (i.e., no fitted models have $\hat{R} \leq c$ for all parameters, where $c$ is the specified convergence threshold) under a given scenario, the scenario will not appear on the x-axis.

# Example: simulations with a fixed-effort design

The previous section outlined the mechanics of using the functions contained in the ValidationExplorer repo. Here, we provide a streamlined example with a fixed-effort validation design, and emphasize how the tool may be used to aid the decision process about the appropriate level of effort for reliable inference. Our example uses the second validation design available in the ValidationExplorer: a fixed-effort design. Under this validation design, a random sample of $p$ percent of all recordings is taken from the first detector night at each site, where $p$ is specified by the user.

We begin the simulation process by selecting the occurrence probabilities $\psi_k$ and relative activity rates $\lambda_k$ for each species $k$. We must also specify the assumed classification probabilities. 

If estimates exist for the species of interest, one option could be to adopt those. For instance, if we assumed an assemblage comprised of *Eptesicus fuscus* (EPFU)*, Lasiurus cinereus* (LACI)*, Lasinoycteris noctivagans* (LANO), and *Myotis lucifugus* (MYLU), we could use posterior estimates obtained by @stratton22. These are shown in Table 1.

```{=latex}
\begin{table}
  \centering
  \begin{tabular} {ccc}
    \toprule 
    Species & $\psi$ & $\lambda$ \\ 
    \midrule
    EPFU    & 0.633  & 5.934 \\
    LACI    & 0.612  & 4.160 \\
    LANO    & 0.849  & 14.25 \\
    MYLU    & 0.898  & 28.25 \\
    \bottomrule
  \end{tabular}
  \caption{Posterior estimates obtained by Stratton et al., (2022)}
  \label{parVals}
\end{table}
```

The classification matrix estimated by @stratton22 was for an assemblage of 11 species, and due to the sum-to-one constraint on the rows, values cannot be borrowed directly. However, we might use these to choose realistic values, starting with the diagonal entries: 
$$\Theta = \begin{bmatrix}
  0.788 & & &\\
   & 0.983 & &\\
   & & 0.983 & \\
   & & & 0.965 \\
\end{bmatrix}.$$

A strategy for filling in off-diagonal entries could be to use the estimates from @stratton22 and augment each value by the difference between each row sum and 1, divided by the number species. For example, we might fill in the first row with the estimates from @stratton22: 

$$\Theta = \begin{bmatrix}
  0.7878 & 0.176 & 0.0245 & 0.0014 \\
  0.0090 & 0.9826 & 0.0018 & 0.0009 \\
  0.0071 & 0.0051 & 0.9829 & 0.0028 \\
  0.0002 & 0.0003 & 0.0002 & 0.9650 \\
\end{bmatrix}.$$

However, the first row sums to 0.788 + 0.176 + 0.0245 + 0.0014 $\approx$ `r round(0.788 + 0.176 + 0.0245 + 0.0014, 3)`. The remaining 0.01 can be divided by four and added to each entry, so that the first row of $\Theta$ is now $(0.7905, 0.1785, 0.027, 0.0039).$ This process can be repeated to obtain an empirically-informed classification matrix. Alternatively, simulated matrices can be simulated using the same procedure as `test_theta2` (Section \@ref:dataSimulation). Using this procedure, one must ensure the entries of the matrix in the first argument of `apply` are adjusted to yield classification matrices that align with expert opinions.

With the parameter values chosen, we begin data simulation. Note that the `scenarios` argument is now a vector, in contrast with the proceeding example.

```{r message=FALSE}
psi <- c(0.633, 0.612, 0.849, 0.898)
lambda <- c(5.934, 4.160, 14.25, 28.25)

Theta_FE <- matrix(
  c(
    0.7905, 0.1785, 0.027, 0.0039, 
    0.010425, 0.984025, 0.00322, 0.002325, 
    0.007625, 0.005625, 0.983425, 0.003325,
    0.008775, 0.008875, 0.008775, 0.973575
  ), 
  nrow = 4, 
  byrow = TRUE
)

nsites <- 100
nvisits <- 4
nspecies <- length(psi)

FE_data <- simulate_validatedData(
  n_datasets = 5, 
  validation_design = "FixedPercent",
  scenarios = c(0.05, .25,  0.5, 0.75), # Note the vector of possible scenarios
  nsites = nsites, 
  nvisits = nvisits, 
  nspecies = nspecies,
  psi = psi, 
  lambda = lambda,
  theta = Theta_FE, 
  save_datasets = TRUE,
  save_masked_datasets = TRUE,
  directory = paste0(here::here("Testing", "FixedEffortExample"))
)
```

Next, we fit the model to the simulated data:

```{r eval=FALSE}
FE_model_fits <- run_sims(
  data_list = FE_data$masked_dfs,
  zeros_list = FE_data$zeros, 
  theta_scenario_id = 1,
  save_fits = FALSE,
  DGVs = list(lambda = lambda, psi = psi, theta = Theta_FE),
  save_individual_summaries_list = FALSE,
  directory = here("Testing", "FixedEffortExample")
)
```

We can visualize the results from the model for each parameter group:

```{r echo=FALSE}
# For faster knitting. Note that eval=FALSE in previous chunk. If you change 
# this setting and refit the model, your results may change! 
#saveRDS(FE_model_fits, paste0(here("Testing", "FixedEffortExample"), "/FE_model_fits.rds"))
FE_model_fits <- readRDS(paste0(here("Testing", "FixedEffortExample"), "/FE_model_fits.rds"))
```

```{r}
visualize_parameter_group(FE_model_fits, pars = "lambda", theta_scenario = 1, scenarios = 1:4)
```

The first fixed effort design, where 5% of all recordings from the first detector-night at each site were validated did not provide enough information for any models fit to the simulated data sets to have $\hat{R} \leq 1.1$ for all parameters after 2000 MCMC iterations. This is shown in the faceted plot of relative activity parameters -- the first manual verification scenario is missing from the $x$-axis. For all other validation scenarios, all parameters in all models converged, as evidenced by the presence of five grey intervals in the background for each parameter-scenario combination. Based on the absence of a red point for any relative activity parameter in any of the validation scenarios, minimal estimation error is expected for relative activity under validation scenarios 2-4. For species 1-3 (i.e., EPFU, LACI, and LANO), the figure shows high coverage. Coverage is slightly low for species 4 (MYLU), but this is likely due to only five datasets being fit for this vignette -- if one 95% posterior interval fails to capture the true parameter value, coverage is reduced to 80%, as in this example figure. When using these software tools for evaluating management decisions, we recommend fitting models to at least 50-100 datasets to ensure that estimates of long-run behavior (e.g., coverage or expected estimation error) are reliable. 

```{r}
visualize_parameter_group(FE_model_fits, pars = "psi", theta_scenario = 1, scenarios = 1:4)
```

The visualization of inference for $\psi_k$ exhibits similar characteristics to the plot for $\lambda,$ especially with regard to convergence of MCMC algorithms. However, we may expect slightly higher estimation error, especially for $\psi_1$, which is the occurrence probability of EPFU. This estimation error does not appear to change across the various validation scenarios. However, coverage for $\psi_1$ improves with greater validation effort (coverage = 80% in scenario 2, but is 100% in scenario 3). Coverage is slightly low for $\psi_2$ across all validation scenarios. Again, these results may change when a greater number of datasets are used for the simulations, which warrants follow-up. If results remain the same after fitting models to 50 or more datasets, one might consider leaning towards validation scenario 3 based on the simulation results for $\psi_1$. 

```{r}
visualize_parameter_group(FE_model_fits, pars = "theta", theta_scenario = 1, scenarios = 1:4)
```

Typically, the misclassification parameters in the $\Theta$ matrix are regarded as nuisance parameters. However, reliable inference for all parameters is desireable, and the visualization of the misclassification parameters further suggests that the 3rd validation scenario may be the best option. Often, this level of validation effort leads to near-nominal coverage of 95% posterior intervals and minimal estimation error, but requires fewer recordings be validated than validation scenario 4.  The exceptions are $\theta_{34}$ (third row, fourth column) and $\theta_{41}$ (fourth row, first column), which have coverage slightly below nominal levels under validation scenario 3.  as noted above this is likely an artifact of the very small number of datasets used in this example.  

We can examine the expected number of validated recordings (for this species assemblage) using the `summarize_n_validated` function: 

```{r}
summarize_n_validated(FE_data$masked_dfs)
```

If the decision is made to use a fixed-effort design, with the third level of validation effort investigated in these scenarios, then around 2200 recordings will need to be validated. If this is infeasible, additional simulations could be undertaken to fine-tune the validation effort to a lower level (somewhere between validation scenario 2 and validation scenario 3) while also ensuring near nominal coverage and minimal estimation error. 

# Conclusion 

We have demonstrated the use of the ValidationExplorer software tool through two examples. Note that with any simulation study, including those conducted using the ValidationExplorer, results are conditional on the settings and assumptions. In the case of the count-detection model framework, the assumptions are 

- The occurrence of species within a site are independent; the presence of one species carries no information about the presence or absence of another. 
- For any one species, its occurrence at one location is independent of its occurrence at any other location (independence across sites).
- Visits to a site (i.e., detector nights) are independent. 
- Recordings within the same site-visit are independent. This holds regardless of whether a recording is validated or remains ambiguous (only has an autoID label). 
- The count of recordings generated by a species in a single detector night is a Poisson random variable. 

\noindent The settings include the number of datasets used in the simulations, the assumed characteristics of the species assemblage (i.e., the values for each $\psi_k$ and $\lambda_k$) and classifier ($\Theta$), and the number of sites and visits. Our hope is that the ValidationExplorer provides a useful tool for assessing the merits of various validation designs, so that effort can be thoughtfully assigned based on program goals and monitoring objectives. Ultimately, we hope that this software streamlines the bat acoustic workflow and allows for efficient processing of information. 

# References

